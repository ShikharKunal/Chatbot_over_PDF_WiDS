# Week 4 & 5

## Coursera Course

To dive deeper into the concepts covered in Week 4 & 5, it's highly recommended to complete the remaining two weeks of the course "Sequence Models" by Andrew Ng on Coursera. You can audit the course for free, and it provides valuable insights into attention mechanisms, transformers, and large language models.

[**Sequence Models on Coursera**](https://www.coursera.org/learn/nlp-sequence-models?)

## Transformers

Transformers represent a breakthrough in neural networks for sequence processing, offering a powerful approach to understanding context in sequences. The following resources will help you grasp the fundamental concepts behind transformers:

- [**The Illustrated Transformer**](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0): A visually informative guide to understanding how transformers work.
- [**StatQuest Video on Transformers**](https://www.youtube.com/watch?v=zxQyTK8quyY): An engaging video explaining transformer concepts.

### Large Language Models (LLMs)

Large Language Models, often based on transformer architectures, have revolutionized natural language processing. They excel in understanding language intricacies, thanks to self-attention mechanisms. Explore more about LLMs with these resources:

- [**Introduction to Large Language Models**](https://research.aimultiple.com/large-language-models/): An introductory guide to understanding the significance of LLMs.
- [**Geeks for Geeks Article on LLM**](https://www.geeksforgeeks.org/large-language-model-llm/): In-depth information on Large Language Models.

### Decoding Strategies

Decoding strategies play a crucial role in generating meaningful sequences from LLMs. Learn about various decoding strategies to refine model outputs:

- [**Hugging Face Blog on Decoding**](https://huggingface.co/blog/how-to-generate): Insights into decoding strategies.
- [**Medium Article on Decoding Strategies**](https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc): A concise guide with helpful graphs.

## Hugging Face ðŸ¤—

Hugging Face is a prominent community in natural language processing, providing tools and resources for researchers and developers. Their **Transformers** library is central to accessing and fine-tuning pre-trained language models.

### ðŸ¤— Transformers

Explore the extensive capabilities of the **Transformers** library:

- [**ðŸ¤— Transformers Documentation**](https://huggingface.co/docs/transformers/index): Comprehensive documentation for the Transformers library.

### UI Options: Streamlit and Gradio

For creating interactive user interfaces, consider using **Streamlit** or **Gradio**. Both are user-friendly options for showcasing models.
Gradio is simpler than Streamlit. 

- **Streamlit:**
  - [**Streamlit Documentation**](https://docs.streamlit.io/): Learn to create interactive web apps with Streamlit.

- **Gradio:**
  - [**Gradio Documentation**](https://www.gradio.app/docs/interface): Explore Gradio's documentation for building intuitive interfaces.

## Transfer Learning and Fine-tuning

Transfer learning and fine-tuning are essential techniques to adapt pre-trained models to specific tasks. Here are resources to guide you through these processes:

- [**Overview of Transfer Learning**](https://medium.com/@atmabodha/pre-training-fine-tuning-and-in-context-learning-in-large-language-models-llms-dd483707b122): A brief theoretical overview.
- [**Stepwise Guide to Fine-tuning LLMs**](https://www.simform.com/blog/completeguide-finetuning-llm/#:~:text=Fine%2Dtuning%20in%20large%20language,your%20specific%20business%20use%20cases.): A step-by-step overview.

For more advanced users interested in building models from scratch, explore [**this YouTube video on building a GPT-style model**](https://youtu.be/kCc8FmEb1nY) (Note: Advanced content beyond the scope of this course). Proceed with caution.

Now you are well-equipped to explore the world of transformers, LLMs, and fine-tuning techniques.

Attempt the optional assignment before proceeding.(_Optional but highly recommended_)
Link to asssignment problem statement:
- [**Optional Assignment**](./Optional_assignment.md)

## Retrieval Augmented Generation (RAG)

RAG is an AI framework for retrieving facts from an external knowledge base to ground large language models (LLMs) on the most accurate, up-to-date information and to give users insight into LLMs' generative process.

- [**Article on RAG**](https://research.ibm.com/blog/retrieval-augmented-generation-RAG)
- Research more about RAG from articles and youtube videos.

## Langchain

LangChain is a framework designed to simplify the creation of applications using large language models. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis

- [**Documentation**](https://python.langchain.com/docs/get_started/introduction)
- [**Video Turorial**](https://youtu.be/_FpT1cwcSLg?si=TA6yxvoe9MZRR0IB)

Go through the documentation(_Recommended_) or the video tutorial(_Simpler to understand_) of langchain and explore how can RAG be implemented using it.

Now you are ready to make your own 'CHATBOT OVER PDF' ;)

(Research about conversational bots if you are interested, how bots retain memory from previous prompts)

Link to final project problem statement:
- [**Final Project**](./Final_Project.md)


### Hint for final project (Give it a look only when you are stuck)
- This is an [**article**](https://medium.com/getpieces/how-to-build-a-langchain-pdf-chatbot-b407fcd750b9) on implementation of a chatbot using langchain which is using RAG under the hood.




